\documentclass[12pt,a4paper]{article}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{graphics, graphicx}
\pagestyle{empty}
\newtheorem{claim}{Claim}

\begin{document}
\textbf{Chapter 6 solutions  \hfill Hanna GÃ¡bor}\\

\begin{enumerate}

\item
\begin{align*}
G_t - V_t(S_t) & = R_{t + 1} + \gamma G_{t + 1} - V_t(S_t) + \gamma V_t(S_{t + 1}) -
\gamma V_t(S_{t + 1}) + \gamma V_{t + 1}(S_{t + 1}) - \gamma V_{t + 1}(S_{t + 1})\\
& = (R_{t + 1} + \gamma V_t(S_{t + 1}) - V_t(S_t)) + (\gamma G_{t + 1} - \gamma V_{t + 1}(S_{t + 1})) + (\gamma V_{t + 1}(S_{t + 1}) -\gamma V_t(S_{t + 1}))\\
& = \delta_t + \gamma(G_{t + 1} - V_{t + 1}(S_{t + 1})) + \gamma \alpha \delta_{t + 1}\\
& = \delta_t + \gamma \alpha \delta_{t + 1} + \gamma \delta_{t + 1} + \gamma^2 \alpha \delta_{t + 2} + \gamma^2(G_{t + 2} - V_{t + 2}(S_{t + 2})) = \dots\\
& = \sum\limits_{k = t}^{T - 1} \gamma^{k-t}( 1 + \alpha) \delta_{k}
\end{align*}

\item
If I get lost a few times at the beginning, then I can see why TD updates can be better.
When I get lost, only the first few states learn that it takes a long time for me to go home. After I reached the highway, the predictions won't change
just because I don't know the way from the new office building to the highway.

\item
The first episode must have terminated on the left with a reward of $-1$.

The other state values didn't change because the error in those cases was\\
$R_{t + 1} + \gamma V(S_{t + 1}) - V(S_t) = 0 + 1 \cdot 0.5 - 0.5 = 0$.

$v(A)$ changed by $\alpha (R_{t + 1} + \gamma V(S_{t + 1}) - V(S_t)) = 0.1 * (-1 + 0 - 0.5) = -0.15$.

\item

\item
For now, let us treat the terminal points as states with values $0$ and $1$ respectively
and all the rewards to be $0$. This doesn't change the updates, but makes the
notation a bit easier.

\begin{claim}
  If all the state values are initialized to $0.5$, then
  \[0 \le V(A) \le V(B) \le V(C) \le V(D) \le V(E) \le 1\] holds throughout the
  run of the TD algorithm.
\end{claim}
\begin{proof}
  The proof goes by induction on the number of steps. The statement clearly holds
  in the beginning. Suppose you are in state $S$ and take a step
  to the right, arriving to $S'$. The updated $V(S)$ then equals to
  \[(1 - \alpha) V(S) + \alpha V(S') \le (1 - \alpha) V(S') + \alpha V(S') = V(S').\]
\end{proof}

For the following claims for a state $S \neq C$ let $S_i$ and $S_o$ denote $S$'s
neighbor closer to $C$ and further from $C$, respectively.

\begin{claim}
  Let $S$ be any state in $\{A, B, D, E\}$. Suppose we are in state $S$ in
  time step $t$ and the previous state was $S_i$. Then
  $|\mathbb{E}(V_{t + 1}(S_t)) - V_{t}(S_o)| < |V_t(S_t) - V_{t}(S_o)|$ if and only if
  \[
  (1 - \alpha) |V_t(S_t) - V_{t - 1} (S_i)| < |V_t(S_o)) - V_t(S_t)|
  \]
\end{claim}
\begin{proof}
  Using the update rule $ V_t(S_i) = (1 - \alpha) V_{t - 1}(S_i) + \alpha V_{t - 1}(S_t)$
  and $V_{t - 1}(S_t) = V_t(S_t)$ the following holds.
  \begin{align*}
  \mathbb{E}(V_{t + 1}(S_t))
  & = V_t(S_t) + \frac{\alpha}{2} \Big(\big(V_t(S_i) - V_t(S_t)\big) + \big(V_t(S_o) - V_t(S_t)\big)\Big)\\
  & = V_t(S_t) + \frac{\alpha}{2} \Big(\big((1 - \alpha) V_{t - 1}(S_i) +
  \alpha V_{t - 1}(S_t) - V_t(S_t)\big) + \big(V_t(S_o) - V_t(S_t)\big)\Big)\\
  & = V_t(S_t) + \frac{\alpha}{2} \Big(((1 - \alpha) \big(V_{t - 1}(S_i) - V_t(S_t)\big)
  + \big(V_t(S_o)) - V_t(S_t)\big)\Big)
  \end{align*}
\end{proof}

Note that if $S_t \in \{A, E\}$, then $S_{t - 1} = S_i$.

\begin{claim}
  Let $S$ be a state in $\{B, D\}$. Suppose we are in state $S$ in
  time step $t$ and the previous state was $S_o$. Then
  $|\mathbb{E}(V_{t + 1}(S_t)) - V_{t - 2}(S_o)| < |V_{t - 2}(S_t) - V_{t - 2}(S_o)|$
  if and only if
  \[
  \]
\end{claim}
\begin{proof}
  First note that $S_{t - 2} = S_t$ and $S_{t - 1} = S_o$. We will use the update rules
  $V_t(S_o) = (1 - \alpha) V_{t - 1}(S_o) + \alpha V_{t - 1}(S_t)$ and
  $V_{t - 1}(S_t) = (1 - \alpha) V_{t - 2}(S_t) + \alpha V_{t - 2}(S_o)$.
  \begin{align*}
  \mathbb{E}(V_{t + 1}(S_t))
%
  & = V_t(S_t) + \frac{\alpha}{2} \Big(\big(V_t(S_i) - V_t(S_t)\big) + \big(V_t(S_o) - V_t(S_t)\big)\Big)\\
%
  & = V_t(S_t) + \frac{\alpha}{2} \Big(\big(V_t(S_i) - V_t(S_t)\big)
  + \big((1 - \alpha) V_{t - 1}(S_o) + \alpha V_{t - 1}(S_t) - V_t(S_t)\big)\Big)\\
%
  & = V_{t - 1}(S_t) + \frac{\alpha}{2} \Big(\big(V_{t - 2}(S_i) - V_{t - 1}(S_t)\big)
  + \big((1 - \alpha) (V_{t - 1}(S_o) - V_{t - 1}(S_t))\big)\Big)\\
%
  & = (1 - \alpha) V_{t - 2}(S_t) + \alpha V_{t - 2}(S_o) \\
  & + \frac{\alpha}{2} \Big(\big(V_{t - 2}(S_i)
  - (1 - \alpha) V_{t - 2}(S_t) - \alpha V_{t - 2}(S_o)\big) \\
  & + \big((1 - \alpha)(V_{t - 2}(S_o)
  - (1 - \alpha) V_{t - 2}(S_t) - \alpha V_{t - 2}(S_o))\big)\Big)\\
%
  & = (1 - \alpha) V_{t - 2}(S_t) + \alpha V_{t - 2}(S_o) \\
  & + \frac{\alpha}{2} \Big(\big(V_{t - 2}(S_i) - V_{t - 2}(S_t)\big)
  + \alpha\big(V_{t - 2}(S_t) - V_{t - 2}(S_o)\big)\\
  & + \big((1 - \alpha)^2(V_{t - 2}(S_o) - V_{t - 2}(S_t))\big)\Big)\\
%
& = V_{t - 2}(S_t) + \frac{\alpha}{2} \Big(\big(V_{t - 2}(S_i) - V_{t - 2}(S_t)\big)
+ \big(((1 - \alpha)^2 - \alpha + 2)(V_{t - 2}(S_o) - V_{t - 2}(S_t))\big)\Big)\\
%
  \end{align*}
\end{proof}


\end{enumerate}
\end{document}
