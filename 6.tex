\documentclass[12pt,a4paper]{article}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{graphics, graphicx}
\pagestyle{empty}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}

\begin{document}
\textbf{Chapter 6 solutions  \hfill Hanna GÃ¡bor}\\

\begin{enumerate}
\item
\begin{align*}
G_t - V_t(S_t) & = R_{t + 1} + \gamma G_{t + 1} - V_t(S_t) + \gamma V_t(S_{t + 1}) -
\gamma V_t(S_{t + 1}) + \gamma V_{t + 1}(S_{t + 1})\\
& - \gamma V_{t + 1}(S_{t + 1})\\
& = (R_{t + 1} + \gamma V_t(S_{t + 1}) - V_t(S_t)) + (\gamma G_{t + 1} - \gamma V_{t + 1}(S_{t + 1}))\\
& + (\gamma V_{t + 1}(S_{t + 1}) -\gamma V_t(S_{t + 1}))\\
& = \delta_t + \gamma(G_{t + 1} - V_{t + 1}(S_{t + 1})) + \gamma \alpha \delta_{t + 1}\\
& = \delta_t + \gamma \alpha \delta_{t + 1} + \gamma \delta_{t + 1} + \gamma^2 \alpha \delta_{t + 2} + \gamma^2(G_{t + 2} - V_{t + 2}(S_{t + 2})) = \dots\\
& = \sum\limits_{k = t}^{T - 1} \gamma^{k-t}( 1 + \alpha) \delta_{k}
\end{align*}

\item
If I get lost a few times at the beginning, then I can see why TD updates can be better.
When I get lost, only the first few states learn that it takes a long time for me to go home. After I reached the highway, the predictions won't change
just because I don't know the way from the new office building to the highway.

\item
The first episode must have terminated on the left with a reward of $-1$.

The other state values didn't change because the error in those cases was\\
$R_{t + 1} + \gamma V(S_{t + 1}) - V(S_t) = 0 + 1 \cdot 0.5 - 0.5 = 0$.

$v(A)$ changed by $\alpha (R_{t + 1} + \gamma V(S_{t + 1}) - V(S_t)) = 0.1 * (-1 + 0 - 0.5) = -0.15$.

\item
I don't think a wider range of $\alpha$ values would have an effect on the results.

\begin{itemize}
\item \textbf{Monte Carlo methods.}
The Monte Carlo version with $\alpha = 0.4$ starts to oscillate pretty fast and doesn't
seem to decrease. I expect a Monte Carlo method with a bigger $\alpha$ value to oscillate
around an error level which is at least as high as the error in the $\alpha = 0.4$
case. The smallest $\alpha$ for the MC method among the examples is $0.1$. That is
the only MC method which we can't see to oscillate. This version does not achieve the
performance of the TD methods. A smaller $\alpha$ value would make the learning progress
even slower. It's possible that the error with Monte Carlo update with $\alpha =<0.1$
goes under the TD error eventually, so looking at more episodes might have an effect
on the results.

\end{itemize}

\item
For now, let us treat the terminal points as states with values $0$ and $1$,
respectively. Let all the rewards be $0$. These changes together don't change
the updates, but make the notation a bit easier. For a state $S \neq C$ let
$S^i$ denote the neighbor of $C$ which is closer to $C$ and $S^o$ denote the
neighbor of $S$ which is further from $C$. Let $V_t(S)$ denote the estimate of
$V(S)$ in time step $t$.

For state $C$, the expected estimated value is $0.5$ by symmetry. We will prove
the following theorem.
\begin{thm}\label{thm}
  Let $S$ be any state in $\{A, B, D, E\}$. Suppose we are in state $S$ in
  time step $t$. Suppose also that
  \[(1 - \alpha) |V_t(S_t) - V_{t - 2} (S^i)| < |V_{t - 2}(S^o)) - V_t(S_t)|.\] Then
  $|\mathbb{E}(V_{t + 1}(S_t)) - V_{t}(S^o)| < |V_t(S_t) - V_{t}(S^o)|$.
\end{thm}

For proving the theorem we need a few lemmas.

\begin{lemma}
  If all the state values are initialized to $0.5$, then
  \[0 \le V_t(A) \le V_t(B) \le V_t(C) \le V_t(D) \le V_t(E) \le 1\] holds throughout the
  run of the TD algorithm.
\end{lemma}
\begin{proof}
  The proof goes by induction on the number of steps. The statement clearly holds
  in the beginning. Suppose you are in state $S$ at time $t$ and take a step
  to the right, arriving to $S'$. Then $V_{t + 1}(S)$ equals to
  \[(1 - \alpha) V_t(S) + \alpha V_t(S') \le (1 - \alpha) V_t(S') + \alpha V_t(S') = V_t(S') = V_{t + 1}(S').\]
  If you step to the left, then
  \[(1 - \alpha) V_t(S) + \alpha V_t(S') \ge (1 - \alpha) V_t(S') + \alpha V_t(S') = V_t(S') = V_{t + 1}(S').\]
\end{proof}

\begin{lemma}
  Let $S$ be any state in $\{A, B, D, E\}$. Suppose we are in state $S$ in
  time step $t$ and the previous state was $S^i$. Then
  $|\mathbb{E}(V_{t + 1}(S_t)) - V_{t}(S^o)| < |V_t(S_t) - V_{t}(S^o)|$ if and only if
  \[
  (1 - \alpha) |V_t(S_t) - V_{t - 1} (S^i)| < |V_t(S^o)) - V_t(S_t)|.
  \]
\end{lemma}
\begin{proof}
  Using the update rule $ V_t(S^i) = (1 - \alpha) V_{t - 1}(S^i) + \alpha V_{t - 1}(S_t)$
  and $V_{t - 1}(S_t) = V_t(S_t)$ the following holds.
  \begin{align*}
  \mathbb{E}(V_{t + 1}(S_t))
  & = V_t(S_t) + \frac{\alpha}{2} \Big(\big(V_t(S^i) - V_t(S_t)\big) + \big(V_t(S^o) - V_t(S_t)\big)\Big)\\
  & = V_t(S_t) + \frac{\alpha}{2} \Big(\big((1 - \alpha) V_{t - 1}(S^i) +
  \alpha V_{t - 1}(S_t) - V_t(S_t)\big)\\
  & + \big(V_t(S^o) - V_t(S_t)\big)\Big)\\
  & = V_t(S_t) + \frac{\alpha}{2} \Big(((1 - \alpha) \big(V_{t - 1}(S^i) - V_t(S_t)\big)\\
  & + \big(V_t(S^o)) - V_t(S_t)\big)\Big)
  \end{align*}
  Hence
  \[|\mathbb{E}(V_{t + 1}(S_t)) - V_{t}(S^o)| < |V_t(S_t) - V_{t}(S^o)|\]
  holds if and only if
  \[|(1 - \alpha) \big(V_{t - 1}(S^i) - V_{t}(S_t)\big)| < |V_t(S^o)) - V_t(S_t)|.\]
\end{proof}

Note that if $S_t \in \{A, E\}$, then $S_{t - 1} = S^i$.

\begin{lemma}
  Let $S$ be a state in $\{B, D\}$. Suppose we are in state $S$ in
  time step $t$ and the previous state was $S^o$. Then
  $|\mathbb{E}(V_{t + 1}(S_t)) - V_{t - 2}(S^o)| < |V_{t - 2}(S_t) - V_{t - 2}(S^o)|$
  if and only if
  \[
  |V_{t - 2}(S_t) - V_{t - 2}(S^i)| <
  ((1 - \alpha)^2 - \alpha + 2)|(V_{t - 2}(S^o) - V_{t - 2}(S_t))|
  \]
\end{lemma}
\begin{proof}
  First note that $S_{t - 2} = S_t$ and $S_{t - 1} = S^o$. We will use the update rules
  $V_t(S^o) = (1 - \alpha) V_{t - 1}(S^o) + \alpha V_{t - 1}(S_t)$ and
  $V_{t - 1}(S_t) = (1 - \alpha) V_{t - 2}(S_t) + \alpha V_{t - 2}(S^o)$.
  \begin{align*}
  \mathbb{E}(V_{t + 1}(S_t))
%
  & = V_t(S_t) + \frac{\alpha}{2} \Big(\big(V_t(S^i) - V_t(S_t)\big) + \big(V_t(S^o) - V_t(S_t)\big)\Big)\\
%
  & = V_t(S_t) + \frac{\alpha}{2} \Big(\big(V_t(S^i) - V_t(S_t)\big)\\
  & + \big((1 - \alpha) V_{t - 1}(S^o) + \alpha V_{t - 1}(S_t) - V_t(S_t)\big)\Big)\\
%
  & = V_{t - 1}(S_t) + \frac{\alpha}{2} \Big(\big(V_{t - 2}(S^i) - V_{t - 1}(S_t)\big)\\
  & + \big((1 - \alpha) (V_{t - 1}(S^o) - V_{t - 1}(S_t))\big)\Big)\\
%
  & = (1 - \alpha) V_{t - 2}(S_t) + \alpha V_{t - 2}(S^o) \\
  & + \frac{\alpha}{2} \Big(\big(V_{t - 2}(S^i)
  - (1 - \alpha) V_{t - 2}(S_t) - \alpha V_{t - 2}(S^o)\big) \\
  & + \big((1 - \alpha)(V_{t - 2}(S^o)
  - (1 - \alpha) V_{t - 2}(S_t) - \alpha V_{t - 2}(S^o))\big)\Big)\\
%
  & = (1 - \alpha) V_{t - 2}(S_t) + \alpha V_{t - 2}(S^o) \\
  & + \frac{\alpha}{2} \Big(\big(V_{t - 2}(S^i) - V_{t - 2}(S_t)\big)
  + \alpha\big(V_{t - 2}(S_t) - V_{t - 2}(S^o)\big)\\
  & + \big((1 - \alpha)^2(V_{t - 2}(S^o) - V_{t - 2}(S_t))\big)\Big)\\
%
& = V_{t - 2}(S_t) + \frac{\alpha}{2} \Big(\big(V_{t - 2}(S^i) - V_{t - 2}(S_t)\big)\\
& + \big(((1 - \alpha)^2 - \alpha + 2)(V_{t - 2}(S^o) - V_{t - 2}(S_t))\big)\Big)\\
%
  \end{align*}
  Hence
  \[|\mathbb{E}(V_{t + 1}(S_t)) - V_{t - 2}(S^o)| < |V_{t - 2}(S_t) - V_{t - 2}(S^o)|\]
  holds if and only if
  \[|V_{t - 2}(S^i) - V_{t - 2}(S_t)|
  < ((1 - \alpha)^2 - \alpha + 2)|(V_{t - 2}(S^o) - V_{t - 2}(S_t))|
  \]
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm}]
  $((1 - \alpha)^2 - \alpha + 2) = 3 - 3\alpha + \alpha^2 \le \frac{1}{1 - \alpha}$.
  \[(3 - 3\alpha + \alpha^2)(1 - \alpha) \le 1\]
  \[3 - 6\alpha - 2\alpha^2 + \alpha^3\]

\end{proof}

\end{enumerate}
\end{document}
