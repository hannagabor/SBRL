\documentclass[12pt,a4paper]{article}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
% \usepackage{graphics, graphicx}
\usepackage{hyperref}
% \usepackage{algorithmic}
% \DeclareMathOperator*{\argmax}{argmax}
% \graphicspath{ {./4.7/} {./4.9/}}
\pagestyle{empty}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
}

\begin{document}
\textbf{Chapter 5 solutions  \hfill Hanna Gábor}

\begin{enumerate}
  \item
    \textit{Consider the diagrams on the right in Figure 5.1. Why does the estimated
    value function jump up for the last two rows in the rear? Why does it drop off for the
    whole last row on the left? Why are the frontmost values higher in the upper diagrams
    than in the lower?}

    In the last two rows we already have 20 or 21 points and we stick with it. We win if
    the dealer goes bust or sticks before reaching 20, so we have a huge probability
    of winning.

    In the last row on the left, the dealer has an ace and that makes is easier to reach
    a high score for the dealer. If he goes over 21 with counting the ace as 11,
    he still has a chance with counting it as 1.

    The frontmost values in the upper diagram are higher than in the lower, because
    having a usable ace increases your chances, as mentioned above.

  \item
    \textit{Suppose every-visit MC was used instead of first-visit MC on the blackjack
      task. Would you expect the results to be very different? Why or why not?}

    I would expect them to be exactly the same. We can't reach the same state twice in a game,
    so the two methods are the same.

  \item
    \textit{What is the backup diagram for Monte Carlo estimation of $q_\pi$?}

    It's the same as for $v_\pi$ on page 95, except for it starts from a state-action pair.

  \item
    \textit{The pseudocode for Monte Carlo ES is inefficient because, for each state–action
    pair, it maintains a list of all returns and repeatedly calculates their mean. It would
    be more efficient to use techniques similar to those explained in Section 2.4 to maintain
    just the mean and a count (for each state–action pair) and update them incrementally.
    Describe how the pseudocode would be altered to achieve this.}

    We can get rid of $Returns$ and introduce $VisitCounts$ that stores an integer
    for each state-action pair. Instead of appending $G$, we would increase the
    corresponding element in $VisitCount$ by one and update $Q(S_t, A_t)$ to
    $(Q(S_t, A_t) \cdot (n - 1) + G)/ n$, where n is $VisitCount(S_t, A_t)$.

  \item
    \textit{Consider an MDP with a single nonterminal state and a single action
    that transitions back to the nonterminal state with probability p and transitions to the
    terminal state with probability 1 - p. Let the reward be +1 on all transitions, and let
    $\gamma = 1$. Suppose you observe one episode that lasts 10 steps, with a return of 10. What
    are the first-visit and every-visit estimators of the value of the nonterminal state?}

    If the estimator takes the average return for the corresponding state, then the
    first-visit estimation of the value of the nonterminal state is $10$, the every-visit
    estimation is $(10 + 9 + ... + 1)/9 = 5.5$.

\end{enumerate}
\end{document}
