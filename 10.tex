\documentclass[12pt,a4paper]{article}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{hyperref}
\usepackage{algorithmic, algorithm}
\usepackage{graphics, graphicx}
\DeclareMathOperator*{\argmax}{argmax}
\pagestyle{empty}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
}

\begin{document}
\textbf{Chapter 10 solutions  \hfill Hanna GÃ¡bor}

\begin{enumerate}
  \item \textit{We have not explicitly considered or given pseudocode for any Monte Carlo
  methods in this chapter. What would they be like? Why is it reasonable not to give
  pseudocode for them? How would they perform on the Mountain Car task?}

  The Monte Carlo is basically $n$-step Sarsa with $n = T$. I expect it to perform
  poorly on the Mountain Car example: until it doesn't reach the goal, it doesn't
  learn anything, so the first episode might be really long.
  As opposed to this, $n$-step Sarsa tries new actions after it sees
  that the previous actions didn't lead to the end of the episode, so it will
  get to the goal line eventually.

  \item \textit{Give pseudocode for semi-gradient one-step Expected Sarsa for control.}

  \begin{algorithm}
    \caption{Semi-gradient expected Sarsa for control}
    \begin{algorithmic}
      \STATE Inputs: a differentiable action-value function parametrization
      $\hat{q}: \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R}$
      \STATE Algorithm parameters: steps size $\alpha > 0, \epsilon > 0$
      \STATE Initialize value-function weights $w \in \mathbb{R}^d$ arbitrarily
      \FOR {each episode}
        \STATE Initialize $S_0$ non-terminal state.
        \FOR {$t = 0, 1, 2 \dots$}
          \STATE Take action $A_t$ according to the $\epsilon$-greedy policy w.r.t.
          $\hat{q}(S_0, \cdot, w)$
          \STATE Observe reward $R_{t + 1}$ and next state $S_{t + 1}$.
          \IF {$S_{t + 1}$ is terminal}
            \STATE $w \leftarrow w + \alpha R_{t + 1} \nabla\hat{q}(S_t, A_t, w)$
            \STATE go to next episode
          \ELSE
            \STATE $w \leftarrow w + \alpha\Big(R_{t + 1} + \sum\limits_a\pi(a|S_{t + 1})
            \hat{q}(S_{t + 1}, a, w) - \hat{q}(S_t, A_t, w)\Big)
            \nabla\hat{q}(S_t, A_t, w)$, where $\pi$ is the $\epsilon$-greedy policy w.r.t.
            $\hat{q}(S_0, \cdot, w)$
            \STATE $S_t \leftarrow S_{t + 1}$
          \ENDIF
        \ENDFOR
      \ENDFOR
    \end{algorithmic}
  \end{algorithm}

  \item \textit{Why do the results shown in Figure 10.4 have higher standard errors at
  large n than at small n?}

  If $n$ is large, it's less predictable where we will end up in $n$ steps. This causes
  our estimates to have a bigger standard deviation. That in turn, will cause the steps
  to be more varied among different runs.
  
  \item \textit{Give pseudocode for a differential version of semi-gradient Q-learning}
   \begin{algorithm}
    \caption{Differential version of semi-gradient Q-learning}
    \begin{algorithmic}
        \STATE Inputs: a differentiable action-value function parametrization
        $\hat{q}: \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R}$
        \STATE Algorithm parameters: steps size $\alpha > 0, \epsilon > 0$
        \STATE Initialize value-function weights $w \in \mathbb{R}^d$ arbitrarily.
        \STATE $\bar{R} \leftarrow 0$
        \STATE $visits \leftarrow 0$
        \FOR {each episode}
          \STATE Initialize $S_0$ non-terminal state.
          \FOR{$t = 0, 1, 2\dots$}
            \STATE Choose action $A_t$ according to the $\epsilon$-greedy policy w.r.t. $\hat{q}$
            \STATE Observe reward $R_{t + 1}$ and next state $S_{t + 1}$.
            \STATE $w \leftarrow w + \alpha(R_{t + 1} - \bar{R} + \max_a(\hat{q}(S_{t + 1}, a, w))
            - \hat{q}(S_t, A_t, w)) \nabla \hat{q}(S_t, A_t, w)$
            \STATE $\bar{R} \leftarrow \frac{visits}{visits + 1} (\bar{R} + R_{t + 1})$
            \STATE $visits \leftarrow visits + 1$
            \IF{$S_{t + 1}$ is terminal}
              \STATE Go to next episode.
            \ENDIF
            \ENDFOR
        \ENDFOR
     \end{algorithmic}
   \end{algorithm}

\end{enumerate}
\end{document}
