\documentclass[12pt,a4paper]{article}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{hyperref}
\usepackage{algorithmic, algorithm}
\usepackage{graphics, graphicx}
\DeclareMathOperator*{\argmax}{argmax}
\pagestyle{empty}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
}

\begin{document}
\textbf{Chapter 10 solutions  \hfill Hanna GÃ¡bor}

\begin{enumerate}
  \item \textit{We have not explicitly considered or given pseudocode for any Monte Carlo
  methods in this chapter. What would they be like? Why is it reasonable not to give
  pseudocode for them? How would they perform on the Mountain Car task?}

  The Monte Carlo is basically $n$-step Sarsa with $n = T$. I expect it to perform
  poorly on the Mountain Car example: until it doesn't reach the goal, it doesn't
  learn anything, so the first episode might be really long.
  As opposed to this, $n$-step Sarsa tries new actions after it sees
  that the previous actions didn't lead to the end of the episode, so it will
  get to the goal line eventually.

  \item \textit{Give pseudocode for semi-gradient one-step Expected Sarsa for control.}

  \begin{algorithm}
    \caption{Semi-gradient expected Sarsa for control}
    \begin{algorithmic}
      \STATE Inputs: a differentiable action-value function parametrization
      $\hat{q}: \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R}$
      \STATE Algorithm parameters: steps size $\alpha > 0, \epsilon > 0$
      \STATE Initialize value-function weights $w \in \mathbb{R}^d$ arbitrarily
      \FOR {each episode}
        \STATE Initialize $S_0$ non-terminal state.
        \FOR {$t = 0, 1, 2 \dots$}
          \STATE Take action $A_t$ according to the $\epsilon$-greedy policy w.r.t.
          $\hat{q}(S_0, \cdot, w)$
          \STATE Observe reward $R_{t + 1}$ and next state $S_{t + 1}$.
          \IF {$S_{t + 1}$ is terminal}
            \STATE $w \leftarrow w + \alpha R_{t + 1} \nabla\hat{q}(S_t, A_t, w)$
            \STATE go to next episode
          \ELSE
            \STATE $w \leftarrow w + \alpha\Big(R_{t + 1} + \sum\limits_a\pi(a|S_{t + 1})
            \hat{q}(S_{t + 1}, a, w) - \hat{q}(S_t, A_t, w)\Big)
            \nabla\hat{q}(S_t, A_t, w)$, where $\pi$ is the $\epsilon$-greedy policy w.r.t.
            $\hat{q}(S_0, \cdot, w)$
            \STATE $S_t \leftarrow S_{t + 1}$
          \ENDIF
        \ENDFOR
      \ENDFOR
    \end{algorithmic}
  \end{algorithm}

  \item \textit{Why do the results shown in Figure 10.4 have higher standard errors at
  large n than at small n?}

  If $n$ is large, it's less predictable where we will end up in $n$ steps. This causes
  our estimates to have a bigger standard deviation. That in turn, will cause the steps
  to be more varied among different runs.
  
  \item \textit{Give pseudocode for a differential version of semi-gradient Q-learning}
  See the algorithm on the next page. I used the average of the seen rewards as $\bar{R}$.
  \begin{algorithm}
    \caption{Differential version of semi-gradient Q-learning}
    \begin{algorithmic}
        \STATE Inputs: a differentiable action-value function parametrization
        $\hat{q}: \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R}$
        \STATE Algorithm parameters: steps size $\alpha > 0, \epsilon > 0$
        \STATE Initialize value-function weights $w \in \mathbb{R}^d$ arbitrarily.
        \STATE $\bar{R} \leftarrow 0$
        \STATE $visits \leftarrow 0$
        \FOR {each episode}
          \STATE Initialize $S_0$ non-terminal state.
          \FOR{$t = 0, 1, 2\dots$}
            \STATE Choose action $A_t$ according to the $\epsilon$-greedy policy w.r.t. $\hat{q}$
            \STATE Observe reward $R_{t + 1}$ and next state $S_{t + 1}$.
            \STATE $w \leftarrow w + \alpha(R_{t + 1} - \bar{R} + \max_a(\hat{q}(S_{t + 1}, a, w))
            - \hat{q}(S_t, A_t, w)) \nabla \hat{q}(S_t, A_t, w)$
            \STATE $\bar{R} \leftarrow \frac{visits}{visits + 1} (\bar{R} + R_{t + 1})$
            \STATE $visits \leftarrow visits + 1$
            \IF{$S_{t + 1}$ is terminal}
              \STATE Go to next episode.
            \ENDIF
            \ENDFOR
        \ENDFOR
     \end{algorithmic}
  \end{algorithm}

  \item \textit{What equations are needed (beyond 10.10) to specify the differential
  version of TD(0)?}

    \begin{align*}
      w_{t + 1} = w_t + \alpha\delta_t \nabla \hat{v}(S_t, w_t)
    \end{align*}

  \item \textit{Suppose there is an MDP that under any policy produces the deterministic
  sequence of rewards +1, 0, +1, 0, +1, 0,... going on forever. Technically, this violates
  ergodicity; there is no stationary limiting distribution $\mu_\pi$ and the limit (10.7) does not
  exist. Nevertheless, the average reward (10.6) is well defined. What is it? Now consider
  two states in this MDP. From A, the reward sequence is exactly as described above,
  starting with a +1, whereas, from B, the reward sequence starts with a 0 and then
  continues with +1, 0, +1, 0,.... We would like to compute the differential values of A and
  B. Unfortunately, the differential return (10.9) is not well defined when starting from
  these states as the implicit limit does not exist. To repair this, one could alternatively
  define the differential value of a state as
  \[ v_\pi (s) = \lim\limits_{\gamma \rightarrow 1} \lim\limits_{h \rightarrow \infty}
  \sum\limits_{t = 0}^h \gamma^t(\mathbb{E}(R_{t + 1} | S_0 = s) - r(\pi)).
  \]
  Under this definition, what are the differential values of states A and B?}

  The average reward is $0.5$.
  \begin{align*}
    \lim\limits_{h \rightarrow \infty}
    \sum\limits_{t = 0}^h \gamma^t(\mathbb{E}(R_{t + 1} | S_0 = A) - r(\pi))
    &= \sum\limits_{t = 0}^\infty \gamma^{2t} - r(\pi) \sum\limits_{t = 0}^\infty \gamma^t
    = \frac{1}{1 - \gamma^2} - \frac{0.5}{1 - \gamma}\\
    v_\pi(A) &= \lim\limits_{\gamma \rightarrow 1} \frac{1}{1 - \gamma^2} - \frac{0.5}{1 - \gamma}
    = \lim\limits_{\gamma \rightarrow 1} \frac{1 - 0.5(1 + \gamma)}{1 - \gamma^2}\\
    &= \lim\limits_{\gamma \rightarrow 1} \frac{0.5(1 - \gamma)}{1 - \gamma^2}
    = \lim\limits_{\gamma \rightarrow 1} 0.5 \frac{1}{1 + \gamma } = 0.25\\
    \lim\limits_{h \rightarrow \infty}
    \sum\limits_{t = 0}^h \gamma^t(\mathbb{E}(R_{t + 1} | S_0 = B) - r(\pi))
    &= \sum\limits_{t = 0}^\infty \gamma^{2t + 1} - r(\pi) \sum\limits_{t = 0}^\infty \gamma^t
    = \frac{1}{1 - \gamma^2} - \frac{0.5}{1 - \gamma}\\
    v_\pi(B) &= \lim\limits_{\gamma \rightarrow 1} \frac{\gamma}{1 - \gamma^2} - \frac{0.5}{1 - \gamma}
    = \lim\limits_{\gamma \rightarrow 1} \frac{\gamma - 0.5(1 + \gamma)}{1 - \gamma^2}\\
    &= \lim\limits_{\gamma \rightarrow 1} \frac{0.5(\gamma - 1)}{1 - \gamma^2}
    = \lim\limits_{\gamma \rightarrow 1} -0.5\frac{1}{1 + \gamma} = -0.25
  \end{align*}

  \item \textit{Consider a Markov reward process consisting of a ring of three states A, B,
  and C, with state transitions going deterministically around the ring. A reward of +1 is
  received upon arrival in A and otherwise the reward is 0. What are the differential values
  of the three states, using (10.13)?}

  $r_\pi = \frac{1}{3}$ in this case.
  \begin{align*}
    \lim\limits_{h \rightarrow \infty}
    \sum\limits_{t = 0}^h \gamma^t(\mathbb{E}(R_{t + 1} | S_0 = A) - r(\pi))
    &= \gamma^2 \sum\limits_{t = 0}^\infty (\gamma^3)^t - r(\pi)
    \sum\limits_{t = 0}^\infty \gamma^t\\
    &= \frac{\gamma^2}{1 - \gamma^3} - \frac{1}{3(1 - \gamma)}\\
    v_\pi(A) &= \lim\limits_{\gamma \rightarrow 1}
    \frac{\gamma^2}{1 - \gamma^3} - \frac{1}{3(1 - \gamma)}\\
    &= \lim\limits_{\gamma \rightarrow 1}
    \frac{3 \gamma^2 - (1 + \gamma + \gamma^2)}{3(1 - \gamma^3)}
    = \lim\limits_{\gamma \rightarrow 1}
    \frac{2\gamma^2 - \gamma - 1}{3(1 - \gamma^3)}\\
    &= \lim\limits_{\gamma \rightarrow 1} \frac{-2 \gamma - 1}{3(1 + \gamma + \gamma^2)}
    = -\frac{1}{3}\\
    \lim\limits_{h \rightarrow \infty}
    \sum\limits_{t = 0}^h \gamma^t(\mathbb{E}(R_{t + 1} | S_0 = B) - r(\pi))
    &= \gamma \sum\limits_{t = 0}^\infty (\gamma^3)^t - r(\pi)
    \sum\limits_{t = 0}^\infty \gamma^t\\
    &= \frac{\gamma}{1 - \gamma^3} - \frac{1}{3(1 - \gamma)}\\
    v_\pi(B) &= \lim\limits_{\gamma \rightarrow 1}
    \frac{\gamma}{1 - \gamma^3} - \frac{1}{3(1 - \gamma)}\\
    &= \lim\limits_{\gamma \rightarrow 1}
    \frac{3 \gamma - (1 + \gamma + \gamma^2)}{3(1 - \gamma^3)}
    = \lim\limits_{\gamma \rightarrow 1}
    \frac{-\gamma^2 + 2 \gamma - 1}{3(1 - \gamma^3)}\\
    &= \lim\limits_{\gamma \rightarrow 1} \frac{\gamma - 1}{3(1 + \gamma + \gamma^2)}
    = 0\\
    \lim\limits_{h \rightarrow \infty}
    \sum\limits_{t = 0}^h \gamma^t(\mathbb{E}(R_{t + 1} | S_0 = C) - r(\pi))
    &= \sum\limits_{t = 0}^\infty (\gamma^3)^t - r(\pi)
    \sum\limits_{t = 0}^\infty \gamma^t\\
    &= \frac{1}{1 - \gamma^3} - \frac{1}{3(1 - \gamma)}\\
    v_\pi(C) &= \lim\limits_{\gamma \rightarrow 1}
    \frac{1}{1 - \gamma^3} - \frac{1}{3(1 - \gamma)}\\
    &= \lim\limits_{\gamma \rightarrow 1}
    \frac{3 - (1 + \gamma + \gamma^2)}{3(1 - \gamma^3)}
    = \lim\limits_{\gamma \rightarrow 1}
    \frac{-\gamma^2 - \gamma  + 2}{3(1 - \gamma^3)}\\
    &= \lim\limits_{\gamma \rightarrow 1} \frac{\gamma + 2}{3(1 + \gamma + \gamma^2)}
    = \frac{1}{3}\\
  \end{align*}
\end{enumerate}
\end{document}
